{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59817cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(file_path, embedding_dim=50):\n",
    "    word2vec = {}\n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            clean_line = line.strip().split()\n",
    "            word, vector = clean_line[0], np.array(clean_line[1:], np.float32)\n",
    "            word2vec[word] = vector\n",
    "            word2index[word] = idx + 1\n",
    "            index2word[idx + 1] = word\n",
    "\n",
    "        vocab_size = len(word2vec) + 1\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim), np.float32)\n",
    "        \n",
    "        for word, idx in word2index.items():\n",
    "            embedding_matrix[idx] = word2vec[word]\n",
    "            \n",
    "    return word2vec, word2index, index2word, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d772a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king 的词向量： [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
      "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
      " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
      " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
      "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
      " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
      " -0.51042 ]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/glove.6B.50d.txt'\n",
    "word2vec, word2index, index2word, embedding_matrix = load_glove_embeddings(file_path)\n",
    "print(\"king 的词向量：\", word2vec.get(\"king\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a3bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe76ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3049af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/ag_news\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# 提取所有文本数据\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a328717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 split 进行分词\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# 生成词汇表\n",
    "counter = Counter()\n",
    "for text in train_text:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "# 生成词汇表，包含特殊 token\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab = special_tokens + [word for word, _ in counter.most_common()]\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dde62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(text):\n",
    "    return torch.tensor([vocab_dict.get(word, vocab_dict[\"<unk>\"]) for word in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "# 生成训练数据（输入 100 个词，预测下一个词）\n",
    "def create_data(text_list, seq_len=100):\n",
    "    X, Y = [], []\n",
    "    for text in text_list:\n",
    "        token_ids = numericalize(text)\n",
    "        if len(token_ids) <= seq_len:\n",
    "            continue  # 忽略过短的文本\n",
    "        for i in range(len(token_ids) - seq_len):\n",
    "            X.append(token_ids[i:i + seq_len])\n",
    "            Y.append(token_ids[i + seq_len])\n",
    "    return torch.stack(X), torch.tensor(Y)\n",
    "\n",
    "# 生成训练数据\n",
    "X_train, Y_train = create_data(train_text, seq_len=100)\n",
    "\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4edd9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(RNNTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)#将输入的单词索引转换为 embed_dim 维的向量。\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)#构建一个 RNN 层，用于处理序列数据。\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)#将 RNN 隐藏状态 映射到 词汇表大小的向量，用于预测下一个单词的概率分布。\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        #输入 x 形状：(batch_size, seq_len)\n",
    "        #输出 embedded 形状：(batch_size, seq_len, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        #输入 embedded 形状：(batch_size, seq_len, embed_dim)\n",
    "        #输出 output 形状：(batch_size, seq_len, hidden_dim)（所有时间步的隐藏状态）\n",
    "        #输出 hidden 形状：(num_layers, batch_size, hidden_dim)（最后一个时间步的隐藏状态）\n",
    "        output, hidden = self.rnn(embedded, hidden) \n",
    "        #只取 最后一个时间步的隐藏状态 output[:, -1, :] 作为输入\n",
    "        #通过全连接层 self.fc 将隐藏状态转换为词汇表大小的分布（用于预测下一个单词）\n",
    "        #最终 output 形状：(batch_size, vocab_size)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "973ccac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 512  \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = RNNTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b5b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  17%|█▋        | 19/111 [00:20<01:41,  1.10s/it, loss=9.79]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Avg Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# 计算并输出本轮训练的平均损失\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;66;03m# 前向传播，计算模型输出\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, Y_batch) \u001b[38;5;66;03m# 计算损失函数值\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# 反向传播，计算梯度\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# 更新模型参数\u001b[39;00m\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;66;03m# 累加当前 batch 的损失值\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    model.train()# 将模型设置为训练模式\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")# 使用 tqdm 创建进度条\n",
    "        epoch_grad_norm = None\n",
    "\n",
    "        for X_batch, Y_batch in progress_bar:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)# 将数据移动到指定设备（GPU/CPU）\n",
    "            optimizer.zero_grad()# 清空上一轮的梯度，防止梯度累积\n",
    "\n",
    "            output, _ = model(X_batch)# 前向传播，计算模型输出\n",
    "            loss = criterion(output, Y_batch) # 计算损失函数值\n",
    "            loss.backward()# 反向传播，计算梯度\n",
    "\n",
    "            optimizer.step() # 更新模型参数\n",
    "            total_loss += loss.item()# 累加当前 batch 的损失值\n",
    "            progress_bar.set_postfix(loss=loss.item())# 在进度条上显示当前 batch 的损失值\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        # 计算并输出本轮训练的平均损失\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e13eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, num_words=100, temperature=1.0):\n",
    "    model.eval()# 将模型设置为评估模式，禁用 dropout 和 batch normalization\n",
    "    words = tokenize(start_text)# 对输入文本进行分词，获取初始词列表\n",
    "    input_seq = numericalize(start_text).unsqueeze(0).to(device)\n",
    "    # 将文本转换为数值表示，并调整形状以符合模型输入格式（增加 batch 维度），再移动到指定设备（CPU/GPU）\n",
    "\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(num_words): # 生成 num_words 个单词\n",
    "        with torch.no_grad(): # 在推理时关闭梯度计算，提高效率\n",
    "            output, hidden = model(input_seq, hidden)# 前向传播，获取模型输出和新的隐藏状态\n",
    "\n",
    "        # 计算 softmax，并应用温度系数\n",
    "        logits = output.squeeze(0) / temperature # 对 logits 除以 temperature 调节概率分布的平滑度\n",
    "        probs = F.softmax(logits, dim=-1) # 计算 softmax 得到概率分布\n",
    "\n",
    "        # 采样新词\n",
    "        predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        # 基于概率分布 随机采样一个词的索引\n",
    "\n",
    "        next_word = vocab[predicted_id]  # 从词表中查找对应的单词\n",
    "        words.append(next_word)# 将生成的单词添加到文本列表中\n",
    "\n",
    "        # 更新输入序列（将新词加入，并移除最旧的词，维持输入长度）\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],\n",
    "                              dim=1)\n",
    "\n",
    "    return \" \".join(words) \n",
    "\n",
    "# 生成文本\n",
    "print(\"\\nGenerated Text:\")\n",
    "test_text = dataset[\"test\"][1][\"text\"]\n",
    "# 取前 100 个单词作为前缀\n",
    "test_prefix = \" \".join(test_text.split()[:100])\n",
    "\n",
    "# 让模型基于该前缀生成 100 个词\n",
    "generated_text = generate_text(model, test_prefix, 100, temperature=0.8)\n",
    "\n",
    "print(\"\\n🔹 模型生成的文本：\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a560327",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 示例用法\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m ppl \u001b[38;5;241m=\u001b[39m compute_perplexity(model, \u001b[43mgenerated_text\u001b[49m, vocab_dict)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity (PPL): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_text' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(model, test_text, vocab_dict, seq_len=100):\n",
    "    \"\"\"\n",
    "    计算给定文本的困惑度（Perplexity, PPL）\n",
    "\n",
    "    :param model: 训练好的语言模型（RNN/LSTM）\n",
    "    :param test_text: 需要评估的文本\n",
    "    :param vocab_dict: 词汇表（用于转换文本到索引）\n",
    "    :param seq_len: 评估时的窗口大小\n",
    "    :return: PPL 困惑度\n",
    "    \"\"\"\n",
    "    model.eval()  # 设为评估模式\n",
    "    words = test_text.lower().split()\n",
    "\n",
    "    # 将文本转换为 token ID，如果词不在词表中，则使用 \"<unk>\"（未知词）对应的索引\n",
    "    token_ids = torch.tensor([vocab_dict.get(word, vocab_dict[\"<unk>\"]) for word in words], dtype=torch.long)\n",
    "\n",
    "    # 计算 PPL\n",
    "    total_log_prob = 0\n",
    "    num_tokens = len(token_ids) - 1  # 预测 num_tokens 次\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_tokens):\n",
    "            \"\"\"遍历文本的每个 token，计算其条件概率，最后累加log概率\"\"\"\n",
    "            input_seq = token_ids[max(0, i - seq_len):i].unsqueeze(0).to(device)  # 获取前 seq_len 个单词\n",
    "            if input_seq.shape[1] == 0:  # 避免 RNN 输入空序列\n",
    "                continue\n",
    "\n",
    "            target_word = token_ids[i].unsqueeze(0).to(device)  # 目标单词\n",
    "\n",
    "            # TODO: 前向传播，预测下一个单词的 logits\n",
    "            output, _ = model(input_seq)\n",
    "\n",
    "            # TODO: 计算 softmax 并取 log 概率\n",
    "            log_probs = F.log_softmax(output, dim=1)\n",
    "\n",
    "            # TODO: 取目标词的对数概率\n",
    "            # 使用 gather 从 log_probs 中精确地取出 target_word 索引对应的概率值\n",
    "            # target_word 形状为 (1,)，需要调整为 (1, 1) 以匹配 gather 的要求\n",
    "            target_log_prob = log_probs.gather(1, target_word.view(-1, 1))\n",
    "\n",
    "            # TODO: 累加 log 概率\n",
    "            # .item() 将单元素张量转换为 Python 数字\n",
    "            total_log_prob += target_log_prob.item()\n",
    "                 \n",
    "\n",
    "    avg_log_prob = total_log_prob / num_tokens  # 计算平均 log 概率\n",
    "    perplexity = torch.exp(torch.tensor(-avg_log_prob)) # 计算 PPL，公式 PPL = exp(-avg_log_prob)\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "ppl = compute_perplexity(model, generated_text, vocab_dict)\n",
    "print(f\"Perplexity (PPL): {ppl:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
