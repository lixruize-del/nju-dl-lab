{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59817cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(file_path, embedding_dim=50):\n",
    "    word2vec = {}\n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            clean_line = line.strip().split()\n",
    "            word, vector = clean_line[0], np.array(clean_line[1:], np.float32)\n",
    "            word2vec[word] = vector\n",
    "            word2index[word] = idx + 1\n",
    "            index2word[idx + 1] = word\n",
    "\n",
    "        vocab_size = len(word2vec) + 1\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim), np.float32)\n",
    "        \n",
    "        for word, idx in word2index.items():\n",
    "            embedding_matrix[idx] = word2vec[word]\n",
    "            \n",
    "    return word2vec, word2index, index2word, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d772a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king çš„è¯å‘é‡ï¼š [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
      "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
      " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
      " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
      "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
      " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
      " -0.51042 ]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/glove.6B.50d.txt'\n",
    "word2vec, word2index, index2word, embedding_matrix = load_glove_embeddings(file_path)\n",
    "print(\"king çš„è¯å‘é‡ï¼š\", word2vec.get(\"king\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a3bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe76ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3049af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/ag_news\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# æå–æ‰€æœ‰æ–‡æœ¬æ•°æ®\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a328717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ split è¿›è¡Œåˆ†è¯\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# ç”Ÿæˆè¯æ±‡è¡¨\n",
    "counter = Counter()\n",
    "for text in train_text:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "# ç”Ÿæˆè¯æ±‡è¡¨ï¼ŒåŒ…å«ç‰¹æ®Š token\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab = special_tokens + [word for word, _ in counter.most_common()]\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dde62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(text):\n",
    "    return torch.tensor([vocab_dict.get(word, vocab_dict[\"<unk>\"]) for word in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "# ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆè¾“å…¥ 100 ä¸ªè¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰\n",
    "def create_data(text_list, seq_len=100):\n",
    "    X, Y = [], []\n",
    "    for text in text_list:\n",
    "        token_ids = numericalize(text)\n",
    "        if len(token_ids) <= seq_len:\n",
    "            continue  # å¿½ç•¥è¿‡çŸ­çš„æ–‡æœ¬\n",
    "        for i in range(len(token_ids) - seq_len):\n",
    "            X.append(token_ids[i:i + seq_len])\n",
    "            Y.append(token_ids[i + seq_len])\n",
    "    return torch.stack(X), torch.tensor(Y)\n",
    "\n",
    "# ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "X_train, Y_train = create_data(train_text, seq_len=100)\n",
    "\n",
    "\n",
    "# åˆ›å»º DataLoader\n",
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4edd9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(RNNTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)#å°†è¾“å…¥çš„å•è¯ç´¢å¼•è½¬æ¢ä¸º embed_dim ç»´çš„å‘é‡ã€‚\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)#æ„å»ºä¸€ä¸ª RNN å±‚ï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®ã€‚\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)#å°† RNN éšè—çŠ¶æ€ æ˜ å°„åˆ° è¯æ±‡è¡¨å¤§å°çš„å‘é‡ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        #è¾“å…¥ x å½¢çŠ¶ï¼š(batch_size, seq_len)\n",
    "        #è¾“å‡º embedded å½¢çŠ¶ï¼š(batch_size, seq_len, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        #è¾“å…¥ embedded å½¢çŠ¶ï¼š(batch_size, seq_len, embed_dim)\n",
    "        #è¾“å‡º output å½¢çŠ¶ï¼š(batch_size, seq_len, hidden_dim)ï¼ˆæ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼‰\n",
    "        #è¾“å‡º hidden å½¢çŠ¶ï¼š(num_layers, batch_size, hidden_dim)ï¼ˆæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼‰\n",
    "        output, hidden = self.rnn(embedded, hidden) \n",
    "        #åªå– æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ output[:, -1, :] ä½œä¸ºè¾“å…¥\n",
    "        #é€šè¿‡å…¨è¿æ¥å±‚ self.fc å°†éšè—çŠ¶æ€è½¬æ¢ä¸ºè¯æ±‡è¡¨å¤§å°çš„åˆ†å¸ƒï¼ˆç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼‰\n",
    "        #æœ€ç»ˆ output å½¢çŠ¶ï¼š(batch_size, vocab_size)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "973ccac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 512  \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = RNNTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b5b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  17%|â–ˆâ–‹        | 19/111 [00:20<01:41,  1.10s/it, loss=9.79]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Avg Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# è®¡ç®—å¹¶è¾“å‡ºæœ¬è½®è®­ç»ƒçš„å¹³å‡æŸå¤±\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# è®­ç»ƒæ¨¡å‹\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;66;03m# å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æ¨¡å‹è¾“å‡º\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, Y_batch) \u001b[38;5;66;03m# è®¡ç®—æŸå¤±å‡½æ•°å€¼\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# æ›´æ–°æ¨¡å‹å‚æ•°\u001b[39;00m\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;66;03m# ç´¯åŠ å½“å‰ batch çš„æŸå¤±å€¼\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    model.train()# å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")# ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡\n",
    "        epoch_grad_norm = None\n",
    "\n",
    "        for X_batch, Y_batch in progress_bar:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)# å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰\n",
    "            optimizer.zero_grad()# æ¸…ç©ºä¸Šä¸€è½®çš„æ¢¯åº¦ï¼Œé˜²æ­¢æ¢¯åº¦ç´¯ç§¯\n",
    "\n",
    "            output, _ = model(X_batch)# å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æ¨¡å‹è¾“å‡º\n",
    "            loss = criterion(output, Y_batch) # è®¡ç®—æŸå¤±å‡½æ•°å€¼\n",
    "            loss.backward()# åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦\n",
    "\n",
    "            optimizer.step() # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "            total_loss += loss.item()# ç´¯åŠ å½“å‰ batch çš„æŸå¤±å€¼\n",
    "            progress_bar.set_postfix(loss=loss.item())# åœ¨è¿›åº¦æ¡ä¸Šæ˜¾ç¤ºå½“å‰ batch çš„æŸå¤±å€¼\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        # è®¡ç®—å¹¶è¾“å‡ºæœ¬è½®è®­ç»ƒçš„å¹³å‡æŸå¤±\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "train_model(model, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e13eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, num_words=100, temperature=1.0):\n",
    "    model.eval()# å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨ dropout å’Œ batch normalization\n",
    "    words = tokenize(start_text)# å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè·å–åˆå§‹è¯åˆ—è¡¨\n",
    "    input_seq = numericalize(start_text).unsqueeze(0).to(device)\n",
    "    # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼Œå¹¶è°ƒæ•´å½¢çŠ¶ä»¥ç¬¦åˆæ¨¡å‹è¾“å…¥æ ¼å¼ï¼ˆå¢åŠ  batch ç»´åº¦ï¼‰ï¼Œå†ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆCPU/GPUï¼‰\n",
    "\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(num_words): # ç”Ÿæˆ num_words ä¸ªå•è¯\n",
    "        with torch.no_grad(): # åœ¨æ¨ç†æ—¶å…³é—­æ¢¯åº¦è®¡ç®—ï¼Œæé«˜æ•ˆç‡\n",
    "            output, hidden = model(input_seq, hidden)# å‰å‘ä¼ æ’­ï¼Œè·å–æ¨¡å‹è¾“å‡ºå’Œæ–°çš„éšè—çŠ¶æ€\n",
    "\n",
    "        # è®¡ç®— softmaxï¼Œå¹¶åº”ç”¨æ¸©åº¦ç³»æ•°\n",
    "        logits = output.squeeze(0) / temperature # å¯¹ logits é™¤ä»¥ temperature è°ƒèŠ‚æ¦‚ç‡åˆ†å¸ƒçš„å¹³æ»‘åº¦\n",
    "        probs = F.softmax(logits, dim=-1) # è®¡ç®— softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ\n",
    "\n",
    "        # é‡‡æ ·æ–°è¯\n",
    "        predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        # åŸºäºæ¦‚ç‡åˆ†å¸ƒ éšæœºé‡‡æ ·ä¸€ä¸ªè¯çš„ç´¢å¼•\n",
    "\n",
    "        next_word = vocab[predicted_id]  # ä»è¯è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„å•è¯\n",
    "        words.append(next_word)# å°†ç”Ÿæˆçš„å•è¯æ·»åŠ åˆ°æ–‡æœ¬åˆ—è¡¨ä¸­\n",
    "\n",
    "        # æ›´æ–°è¾“å…¥åºåˆ—ï¼ˆå°†æ–°è¯åŠ å…¥ï¼Œå¹¶ç§»é™¤æœ€æ—§çš„è¯ï¼Œç»´æŒè¾“å…¥é•¿åº¦ï¼‰\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],\n",
    "                              dim=1)\n",
    "\n",
    "    return \" \".join(words) \n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "print(\"\\nGenerated Text:\")\n",
    "test_text = dataset[\"test\"][1][\"text\"]\n",
    "# å–å‰ 100 ä¸ªå•è¯ä½œä¸ºå‰ç¼€\n",
    "test_prefix = \" \".join(test_text.split()[:100])\n",
    "\n",
    "# è®©æ¨¡å‹åŸºäºè¯¥å‰ç¼€ç”Ÿæˆ 100 ä¸ªè¯\n",
    "generated_text = generate_text(model, test_prefix, 100, temperature=0.8)\n",
    "\n",
    "print(\"\\nğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a560327",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ç¤ºä¾‹ç”¨æ³•\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m ppl \u001b[38;5;241m=\u001b[39m compute_perplexity(model, \u001b[43mgenerated_text\u001b[49m, vocab_dict)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity (PPL): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_text' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(model, test_text, vocab_dict, seq_len=100):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç»™å®šæ–‡æœ¬çš„å›°æƒ‘åº¦ï¼ˆPerplexity, PPLï¼‰\n",
    "\n",
    "    :param model: è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹ï¼ˆRNN/LSTMï¼‰\n",
    "    :param test_text: éœ€è¦è¯„ä¼°çš„æ–‡æœ¬\n",
    "    :param vocab_dict: è¯æ±‡è¡¨ï¼ˆç”¨äºè½¬æ¢æ–‡æœ¬åˆ°ç´¢å¼•ï¼‰\n",
    "    :param seq_len: è¯„ä¼°æ—¶çš„çª—å£å¤§å°\n",
    "    :return: PPL å›°æƒ‘åº¦\n",
    "    \"\"\"\n",
    "    model.eval()  # è®¾ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    words = test_text.lower().split()\n",
    "\n",
    "    # å°†æ–‡æœ¬è½¬æ¢ä¸º token IDï¼Œå¦‚æœè¯ä¸åœ¨è¯è¡¨ä¸­ï¼Œåˆ™ä½¿ç”¨ \"<unk>\"ï¼ˆæœªçŸ¥è¯ï¼‰å¯¹åº”çš„ç´¢å¼•\n",
    "    token_ids = torch.tensor([vocab_dict.get(word, vocab_dict[\"<unk>\"]) for word in words], dtype=torch.long)\n",
    "\n",
    "    # è®¡ç®— PPL\n",
    "    total_log_prob = 0\n",
    "    num_tokens = len(token_ids) - 1  # é¢„æµ‹ num_tokens æ¬¡\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_tokens):\n",
    "            \"\"\"éå†æ–‡æœ¬çš„æ¯ä¸ª tokenï¼Œè®¡ç®—å…¶æ¡ä»¶æ¦‚ç‡ï¼Œæœ€åç´¯åŠ logæ¦‚ç‡\"\"\"\n",
    "            input_seq = token_ids[max(0, i - seq_len):i].unsqueeze(0).to(device)  # è·å–å‰ seq_len ä¸ªå•è¯\n",
    "            if input_seq.shape[1] == 0:  # é¿å… RNN è¾“å…¥ç©ºåºåˆ—\n",
    "                continue\n",
    "\n",
    "            target_word = token_ids[i].unsqueeze(0).to(device)  # ç›®æ ‡å•è¯\n",
    "\n",
    "            # TODO: å‰å‘ä¼ æ’­ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„ logits\n",
    "            output, _ = model(input_seq)\n",
    "\n",
    "            # TODO: è®¡ç®— softmax å¹¶å– log æ¦‚ç‡\n",
    "            log_probs = F.log_softmax(output, dim=1)\n",
    "\n",
    "            # TODO: å–ç›®æ ‡è¯çš„å¯¹æ•°æ¦‚ç‡\n",
    "            # ä½¿ç”¨ gather ä» log_probs ä¸­ç²¾ç¡®åœ°å–å‡º target_word ç´¢å¼•å¯¹åº”çš„æ¦‚ç‡å€¼\n",
    "            # target_word å½¢çŠ¶ä¸º (1,)ï¼Œéœ€è¦è°ƒæ•´ä¸º (1, 1) ä»¥åŒ¹é… gather çš„è¦æ±‚\n",
    "            target_log_prob = log_probs.gather(1, target_word.view(-1, 1))\n",
    "\n",
    "            # TODO: ç´¯åŠ  log æ¦‚ç‡\n",
    "            # .item() å°†å•å…ƒç´ å¼ é‡è½¬æ¢ä¸º Python æ•°å­—\n",
    "            total_log_prob += target_log_prob.item()\n",
    "                 \n",
    "\n",
    "    avg_log_prob = total_log_prob / num_tokens  # è®¡ç®—å¹³å‡ log æ¦‚ç‡\n",
    "    perplexity = torch.exp(torch.tensor(-avg_log_prob)) # è®¡ç®— PPLï¼Œå…¬å¼ PPL = exp(-avg_log_prob)\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "ppl = compute_perplexity(model, generated_text, vocab_dict)\n",
    "print(f\"Perplexity (PPL): {ppl:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
