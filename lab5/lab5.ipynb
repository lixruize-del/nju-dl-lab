{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fbb0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8080ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **1. 加载 AG NEWS 数据集**\n",
    "df = pd.read_csv('data/train.csv')  # 请替换成你的文件路径\n",
    "df.columns = [\"label\", \"title\", \"description\"]  # CSV 有3列: 标签, 标题, 描述\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]  # 合并标题和描述作为输入文本\n",
    "df[\"label\"] = df[\"label\"] - 1  # AG NEWS 的标签是 1-4，我们转换成 0-3\n",
    "train_texts, train_labels = df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "number = int(0.3 * len(train_texts))\n",
    "train_texts, train_labels = train_texts[: number], train_labels[: number]\n",
    "\n",
    "df = pd.read_csv('data/test.csv')  # 请替换成你的文件路径\n",
    "df.columns = [\"label\", \"title\", \"description\"]  # CSV 有3列: 标签, 标题, 描述\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]  # 合并标题和描述作为输入文本\n",
    "df[\"label\"] = df[\"label\"] - 1  # AG NEWS 的标签是 1-4，我们转换成 0-3\n",
    "test_texts, test_labels = df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "# **2. 加载 BERT Tokenizer**\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# **3. 处理数据**\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=50):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        return input_ids, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "unk_idx = tokenizer.unk_token_id\n",
    "\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e46875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这段代码是 Transformer中的位置编码（PositionalEncoding），用于给输入的 token embedding 加入位置信息。\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # 创建一个全0的矩阵，shape = (max_len, d_model)\n",
    "        # 表示每个位置 (0 ~ max_len-1) 对应的 d_model 维位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # 生成位置索引，shape = (max_len, 1)\n",
    "        # 即 position = [0, 1, 2, ..., max_len-1] 的列向量\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        # TODO 1: 计算 div_term，用于控制不同维度的 sin/cos 频率\n",
    "        # 要求: 使用 torch.exp() 实现 1 / 10000^(2i/d_model)\n",
    "        div_term = torch.exp(-torch.arange(0, d_model, 2) * (torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        # TODO 2: 给偶数维度位置编码赋值\n",
    "        # 要求: 使用 torch.sin() 完成 position * div_term，赋值给 pe 的偶数列\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # TODO 3: 给奇数维度位置编码赋值\n",
    "        # 要求: 使用 torch.cos() 完成 position * div_term，赋值给 pe 的奇数列\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 将 pe 注册为 buffer（不会被训练优化）\n",
    "        # 并扩展成 (1, max_len, d_model) 方便后续和 batch 做广播\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 是输入的 embedding，shape = (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 将对应位置的 pe 加到 x 上\n",
    "        # self.pe[:, :x.size(1)] shape = (1, seq_len, d_model) 自动广播到 batch_size\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "\n",
    "        # 返回位置编码后的 embedding\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01f063fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Self-Attention 的完整实现\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        # 保证 d_model 可以被 n_heads 整除，方便分头\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads # 每个 head 的特征维度\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 共享一个 Linear 层同时生成 Q, K, V\n",
    "        self.qkv_linear = nn.Linear(d_model, d_model * 3) # 输出为 [Q; K; V]\n",
    "\n",
    "        # 输出层，将多头的结果重新映射回 d_model 维度\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 输入 x: (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # 一次性计算 Q、K、V，输出 shape = (batch_size, seq_len, 3 * d_model)\n",
    "        qkv = self.qkv_linear(x)\n",
    "\n",
    "        # 切分成 n_heads 个 head，准备 multi-head attention\n",
    "        # shape 变为 (batch_size, seq_len, n_heads, 3 * d_k)\n",
    "        qkv = qkv.view(batch_size, seq_len, self.n_heads, 3 * self.d_k)\n",
    "\n",
    "        # 调整维度顺序，变成 (batch_size, n_heads, seq_len, 3 * d_k)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # (batch_size, n_heads, seq_len, 3*d_k)\n",
    "\n",
    "        # 沿最后一个维度切成 Q, K, V，shape = (batch_size, n_heads, seq_len, d_k)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # (batch_size, n_heads, seq_len, d_k)\n",
    "\n",
    "        # TODO 1: 计算 attention scores\n",
    "        # 要求: 使用缩放点积的方式计算 (Q x K^T)，并除以 sqrt(d_k)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) /torch.sqrt(torch.tensor(self.d_k, dtype = torch.float32))\n",
    "\n",
    "        # mask 操作，屏蔽掉 padding 部分\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # TODO 2: 计算 attention 权重\n",
    "        # 要求: 在 seq_len 维度上使用 softmax 归一化 scores\n",
    "        attn = torch.softmax(scores, dim = -1)\n",
    "\n",
    "        # TODO 3: 计算加权求和后的 context\n",
    "        # 要求: 用 attn 加权 V，得到 context\n",
    "        context = torch.matmul(attn, v)\n",
    "\n",
    "        # 将多头拼接回去，shape = (batch_size, seq_len, n_heads * d_k) = (batch_size, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "\n",
    "        # 通过输出层，再映射回原始 d_model 维度\n",
    "        return self.fc(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38a3e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        # 多头自注意力模块，输入输出维度都是 d_model\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, n_heads)\n",
    "\n",
    "        # 前馈全连接层，包含两层线性 + ReLU\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        # 第一层 LayerNorm，作用在自注意力的残差连接之后\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        # 第二层 LayerNorm，作用在前馈网络的残差连接之后\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # ------------------ 自注意力块 ------------------ #\n",
    "\n",
    "        # TODO 1: 计算多头自注意力输出 x2\n",
    "        x2 = self.self_attn(x, mask)\n",
    "\n",
    "        # TODO 2: 残差连接 + 第一层 LayerNorm\n",
    "        x = self.norm1(x + x2)\n",
    "\n",
    "        # ------------------ 前馈神经网络块 ------------------ #\n",
    "\n",
    "        # TODO 3: 前馈全连接网络（两层 Linear + ReLU）得到 x2\n",
    "        x2 = self.ff(x)\n",
    "\n",
    "        # TODO 4: 残差连接 + 第二层 LayerNorm\n",
    "        x = self.norm2(x + x2)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daffcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, d_ff=256, num_layers=2, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. 定义词嵌入层（Embedding），输入为词表大小，输出为 d_model 维\n",
    "        # padding_idx 用于指定 padding token 的索引，避免其被训练\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "\n",
    "        # 2. 定义位置编码器，为 token embedding 添加位置信息\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # 3. 定义多个 TransformerEncoderLayer 叠加起来，num_layers 为层数\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "\n",
    "        # 4. 定义输出分类层，将 encoder 最终输出映射到 num_classes 维度\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)，输入为单词 ID 序列\n",
    "\n",
    "        # 1. 输入 token ID 通过 Embedding，转成 (batch_size, seq_len, d_model) 的 dense 向量\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 2. 加入位置编码，增强位置感知能力\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # 3. 创建 padding mask，shape: (batch_size, 1, 1, seq_len)\n",
    "        # mask = True 代表有效 token，False 代表 padding 位置\n",
    "        pad_mask = (x.sum(-1) != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "        # 4. 依次通过多层 Encoder，每一层都会使用 pad_mask\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, pad_mask)\n",
    "\n",
    "        # 5. 对时间维度（seq_len）做 mean pooling，聚合所有位置的特征\n",
    "        out = x.mean(dim=1)  # mean pooling on seq_len\n",
    "\n",
    "        # 6. 分类输出，映射到类别数\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c734a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 split 进行分词\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerEncoderClassifier(len(vocab)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "249c285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
    "    for text, labels in loop:\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 更新tqdm进度条\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0152ffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     acc \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(text)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(test_dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for text, labels in loop:\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            output = model(text)\n",
    "            preds = output.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    loss = train_epoch()\n",
    "    acc = evaluate()\n",
    "    print(f'Epoch {epoch}: Loss = {loss:.4f}, Test Acc = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6391ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b414b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a72cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 创建模型\n",
    "model = SimpleModel()\n",
    "\n",
    "# 将模型放到GPU（如果可用）\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 训练示例\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # 将数据放到GPU（如果可用）\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c681cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "watch -n 5 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabe064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# 指定模型名称\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# 读取模型对应的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 载入模型\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 输入文本\n",
    "input_text = \"Here is some text to encode\"\n",
    "\n",
    "# 通过tokenizer把文本变成 token_id\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "print(input_ids)\n",
    "\n",
    "# 转换为Tensor\n",
    "input_ids = torch.tensor([input_ids])\n",
    "\n",
    "# 获得BERT的输出\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "\n",
    "# 获得BERT模型最后一个隐层结果\n",
    "output_hidden_states = output.last_hidden_state\n",
    "output_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# 指定模型名称\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# 读取模型对应的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 载入模型\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 打印模型所有参数的名称和形状\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# **1. 加载 AG NEWS 数据集**\n",
    "df = pd.read_csv(\"train.csv\")  # 请替换成你的文件路径\n",
    "df.columns = [\"label\", \"title\", \"description\"]  # CSV 有3列: 标签, 标题, 描述\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]  # 合并标题和描述作为输入文本\n",
    "df[\"label\"] = df[\"label\"] - 1  # AG NEWS 的标签是 1-4，我们转换成 0-3\n",
    "train_texts, train_labels = df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "number = int(0.3 * len(train_texts))\n",
    "train_texts, train_labels = train_texts[: number], train_labels[: number]\n",
    "\n",
    "df = pd.read_csv(\"test.csv\")  # 请替换成你的文件路径\n",
    "df.columns = [\"label\", \"title\", \"description\"]  # CSV 有3列: 标签, 标题, 描述\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]  # 合并标题和描述作为输入文本\n",
    "df[\"label\"] = df[\"label\"] - 1  # AG NEWS 的标签是 1-4，我们转换成 0-3\n",
    "test_texts, test_labels = df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "# **2. 加载 BERT Tokenizer**\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# **3. 处理数据**\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=50):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        } # 此处会自动生成BERT输入所需要的attention_mask\n",
    "\n",
    "\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# **4. 定义和加载BERT分类模型**\n",
    "#TODO:定义模型并且放到GPU上\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert =\n",
    "        self.classifier = \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = BERTClassifier(model_name, num_labels=4).to(device)\n",
    "\n",
    "\n",
    "# **5. 设置优化器和损失函数**\n",
    "#TODO: 定义优化器和损失函数\n",
    "\n",
    "# **6. 训练 BERT**\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in loop:\n",
    "        #TODO: 基于后面需要打印的损失，定义训练过程\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "    # **7. 评估模型**\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            #TODO: 基于后面计算acc需要的true_labels和preds，完善下面测试代码\n",
    "\n",
    "\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
